# ColeAI ü§ñ

**Production-Grade RAG System with Cole Medin's Best Practices**

Multi-persona knowledge bank powered by **PydanticAI**, **PostgreSQL pgvector**, and **advanced RAG strategies**. Create AI agents from any person's public content with state-of-the-art retrieval quality.

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![PostgreSQL](https://img.shields.io/badge/postgresql-14+-blue.svg)](https://www.postgresql.org/)
[![pgvector](https://img.shields.io/badge/pgvector-0.5.0+-blue.svg)](https://github.com/pgvector/pgvector)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## üéØ What Makes This Special?

This project implements **production RAG best practices** from [Cole Medin's mcp-crawl4ai-rag](https://github.com/coleam00/mcp-crawl4ai-rag), featuring:

- ‚úÖ **Header-Based Chunking** - Intelligent text segmentation by markdown/HTML headers (better semantic boundaries)
- ‚úÖ **Code Block Extraction** - Separate indexing of code examples with LLM-generated summaries
- ‚úÖ **CrossEncoder Reranking** - Post-retrieval scoring for improved relevance (3x fetch ‚Üí rerank ‚Üí top K)
- ‚úÖ **Batch Embeddings** - Exponential backoff retry logic with fault tolerance
- ‚úÖ **Toggleable Strategies** - Independent configuration of each RAG enhancement
- ‚úÖ **Source Filtering** - Precision queries by URL/domain

### Performance Metrics

| Configuration | Response Time | Quality |
|--------------|--------------|---------|
| Basic RAG | 34.38s | Good |
| Enhanced RAG (reranking only) | 48.10s | Better |
| **Enhanced RAG (full)** | **46.31s** | **Best** |

**Result**: 85% more semantic chunks (61 vs 33), 17 code blocks with summaries, significantly improved retrieval quality with acceptable latency increase.

---

## üöÄ Features

### Core Capabilities
- ü§ñ **PydanticAI Agents** - Query persona knowledge with natural language using Claude 3 Opus
- üîç **Advanced RAG** - Header-based chunking, code extraction, and CrossEncoder reranking
- üìä **pgvector Search** - Semantic similarity search with cosine distance
- üï∑Ô∏è **Pluggable Crawlers** - YouTube, GitHub, Twitter (extensible architecture)
- üéØ **Dual Interfaces** - MCP server (Claude Code) + HTTP API (Telegram, custom apps)
- üß† **Wisdom Extraction** - Distill recurring themes and principles from content
- ‚ôªÔ∏è **Repeatable Pipeline** - Build new personas with a single script

### RAG Enhancements (Toggleable)

All strategies can be independently enabled/disabled via environment variables:

| Strategy | Feature | Impact | Cost |
|----------|---------|--------|------|
| **Header Chunking** | `USE_HEADER_CHUNKING=true` | Better semantic boundaries | No extra cost |
| **Code Extraction** | `USE_CODE_EXTRACTION=true` | Improved code search | +1 LLM call per code block (indexing) |
| **Reranking** | `USE_RERANKING=true` | Better relevance | Local model (no API cost) |
| **Contextual Embeddings** | `USE_CONTEXTUAL_EMBEDDINGS=true` | Enhanced retrieval | +1 LLM call per chunk (indexing) |

---

## üì¶ Installation

### Prerequisites

- **Python 3.11+**
- **PostgreSQL 14+** with **pgvector** extension
- **Docker** (recommended for PostgreSQL)
- **Poetry** for dependency management

### API Keys Required

- **OpenAI API Key** - For embeddings (`text-embedding-3-small`)
- **Anthropic API Key** - For PydanticAI agent (Claude 3 Opus)
- **GitHub Token** - For crawling repositories

### Setup Steps

1. **Clone repository**:
```bash
git clone https://github.com/gpt153/ColeAI.git
cd ColeAI
```

2. **Install dependencies**:
```bash
poetry install
```

3. **Configure environment**:
```bash
cp .env.example .env
# Edit .env with your API keys and RAG configuration
```

4. **Start PostgreSQL with pgvector**:
```bash
docker-compose up -d
```

5. **Initialize database**:
```bash
psql $DATABASE_URL < migrations/001_init_schema.sql
```

### Docker Installation (Recommended)

The easiest way to run ColeAI is using Docker Compose, which handles both the application and PostgreSQL setup.

#### Option A: Pre-built Image (Fastest) ‚≠ê

Use the pre-built image from GitHub Container Registry - no local build required!

**System Requirements**:
- Docker 20.10+ with Docker Compose V2
- At least 8GB RAM
- At least 2GB free disk space (just to pull the image)

**Steps:**

1. **Clone repository**:
```bash
git clone https://github.com/gpt153/ColeAI.git
cd ColeAI
```

2. **Configure API keys**:
```bash
cp docker-compose.override.yml.example docker-compose.override.yml
# Edit docker-compose.override.yml with your API keys
```

3. **Start services** (uses pre-built image):
```bash
docker compose -f docker-compose.prebuilt.yml up -d
```

4. **Check logs**:
```bash
docker compose -f docker-compose.prebuilt.yml logs -f app
```

5. **Access**:
- API: http://localhost:8000
- Docs: http://localhost:8000/docs

---

#### Option B: Build Locally

Build the Docker image yourself. Requires more disk space and time.

**System Requirements**:
- Docker 20.10+ with Docker Compose V2
- At least 8GB RAM
- At least 10GB free disk space (for build process)

1. **Clone repository**:
```bash
git clone https://github.com/gpt153/ColeAI.git
cd ColeAI
```

2. **Configure API keys**:
```bash
cp docker-compose.override.yml.example docker-compose.override.yml
# Edit docker-compose.override.yml with your API keys:
# - OPENAI_API_KEY
# - ANTHROPIC_API_KEY
# - GITHUB_TOKEN
```

3. **Start all services**:
```bash
docker-compose up -d --build
```

4. **Check logs**:
```bash
# View all logs
docker-compose logs -f

# View app logs only
docker-compose logs -f app

# View postgres logs only
docker-compose logs -f postgres
```

5. **Access the application**:
- **HTTP API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

6. **Initialize with test data** (optional):
```bash
# Run test script inside container
docker-compose exec app python scripts/test_github_only.py
```

7. **Stop services**:
```bash
docker-compose down
```

**What's included**:
- ‚úÖ PostgreSQL 16 with pgvector extension
- ‚úÖ Automatic database initialization (migrations run on first start)
- ‚úÖ FastAPI server on port 8000
- ‚úÖ MCP server on port 8100
- ‚úÖ Health checks and automatic restarts
- ‚úÖ Persistent data storage (Docker volumes)

---

## ‚öôÔ∏è Configuration

### RAG Strategy Configuration

Edit `.env` to enable/disable RAG enhancements:

```env
# ===== RAG Strategy Toggles =====

# Header-based chunking (recommended for production)
USE_HEADER_CHUNKING=true

# Code extraction with LLM summaries (recommended for code-heavy content)
USE_CODE_EXTRACTION=true

# CrossEncoder reranking (recommended for production)
USE_RERANKING=true

# Contextual embeddings (optional, requires reindexing)
USE_CONTEXTUAL_EMBEDDINGS=false

# ===== RAG Settings =====

# LLM for contextual embeddings and code summaries
CONTEXT_MODEL=gpt-4o-mini

# CrossEncoder model for reranking
RERANKING_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Minimum code block length for extraction (characters)
MIN_CODE_BLOCK_LENGTH=300

# Embedding model
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSION=1536

# Chunking settings (for fixed-size fallback)
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

### Recommended Configurations

**1. Development (Fast)**
```env
USE_HEADER_CHUNKING=false
USE_CODE_EXTRACTION=false
USE_RERANKING=false
USE_CONTEXTUAL_EMBEDDINGS=false
```

**2. Production (Balanced)** ‚≠ê Recommended
```env
USE_HEADER_CHUNKING=true
USE_CODE_EXTRACTION=true
USE_RERANKING=true
USE_CONTEXTUAL_EMBEDDINGS=false
```

**3. Maximum Quality (Slow)**
```env
USE_HEADER_CHUNKING=true
USE_CODE_EXTRACTION=true
USE_RERANKING=true
USE_CONTEXTUAL_EMBEDDINGS=true  # Requires reindexing!
```

---

## üéì Build Your First Persona (ColeAI)

### Option 1: Quick Test (Single GitHub Repo)

```bash
poetry run python scripts/test_github_only.py
```

This creates a test persona with:
- Persona: `test-dev`
- Source: anthropic-sdk-python README
- Purpose: Quick RAG testing

### Option 2: Full ColeAI Persona

```bash
poetry run python scripts/build_coleai.py
```

This will:
- Create Cole Medin persona (`cole-medin`)
- Crawl YouTube AI Agents Masterclass
- Crawl GitHub repos (ai-agents-masterclass, context-engineering-intro)
- Generate embeddings with enhanced RAG strategies
- Extract wisdom on context engineering

### Rebuild with Enhanced Strategies

After changing RAG configuration in `.env`, rebuild the knowledge base:

```bash
poetry run python scripts/rebuild_with_enhanced_rag.py
```

This will:
1. Clear existing content chunks
2. Re-crawl all sources
3. Apply new chunking strategy (header-based or size-based)
4. Extract and index code blocks with LLM summaries
5. Generate embeddings with optional contextual enhancement

**Output Example**:
```
================================================================================
‚úÖ REBUILD COMPLETE!
================================================================================
üìä Statistics:
  - Total text chunks: 61
  - Total code blocks: 17
  - Total indexed items: 78
  - Chunking strategy: Header-based
  - Code extraction: Enabled
```

---

## üîç Query ColeAI

### Via HTTP API

```bash
# Start API server
poetry run uvicorn src.api.app:app --reload --port 8000

# Query the persona
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "persona_slug": "test-dev",
    "question": "Show me how to make async API calls with the Anthropic SDK"
  }'
```

**Response includes**:
- AI-generated answer
- Source citations with URLs
- Reranked context chunks (if reranking enabled)

### Via MCP Server (Claude Code)

```bash
# Start MCP server on port 8100
poetry run python src/mcp_server/server.py
```

Add to your Claude Code MCP config (`~/.config/claude-code/mcp_config.json`):
```json
{
  "mcpServers": {
    "coleai": {
      "command": "poetry",
      "args": ["run", "python", "src/mcp_server/server.py"],
      "cwd": "/path/to/ColeAI"
    }
  }
}
```

Then in Claude Code:
```
query_persona_knowledge("test-dev", "What is context engineering?")
```

---

## üß™ Testing RAG Strategies

Test and compare different RAG configurations:

```bash
poetry run python scripts/test_rag_strategies.py
```

**Output**:
```
================================================================================
TEST QUESTION: Show me how to make async API calls with the Anthropic SDK
================================================================================

üìä Current RAG Configuration:
  - Contextual Embeddings: false
  - Reranking: true
  - Code Extraction: true
  - Header Chunking: true
  - Context Model: gpt-4o-mini
  - Reranking Model: cross-encoder/ms-marco-MiniLM-L-6-v2

üöÄ Querying with current RAG configuration...

================================================================================
‚úÖ RESPONSE (took 46.31s):
================================================================================
[AI response with code examples and citations]
```

The script provides recommendations for optimal RAG configurations based on your use case.

---

## üèóÔ∏è Architecture

### Tech Stack

- **PydanticAI** - Type-safe agent framework with Claude 3 Opus
- **PostgreSQL + pgvector** - Vector database for similarity search
- **OpenAI Embeddings** - `text-embedding-3-small` (1536 dimensions)
- **sentence-transformers** - CrossEncoder reranking (`cross-encoder/ms-marco-MiniLM-L-6-v2`)
- **FastAPI** - HTTP API server
- **MCP Protocol** - Claude Code integration
- **Docker Compose** - Container orchestration

### Project Structure

```
ColeAI/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents/              # PydanticAI persona agents
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ persona_agent.py # Query handler with RAG
‚îÇ   ‚îú‚îÄ‚îÄ api/                 # FastAPI HTTP endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ   ‚îú‚îÄ‚îÄ crawlers/            # Content crawlers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ github.py        # GitHub repository crawler
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ youtube.py       # YouTube transcript crawler
‚îÇ   ‚îú‚îÄ‚îÄ mcp_server/          # MCP server for Claude Code
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ server.py
‚îÇ   ‚îú‚îÄ‚îÄ vector_store/        # RAG implementation ‚≠ê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunking.py      # Header-based chunking + code extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py    # Batch embeddings with retry logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reranking.py     # CrossEncoder reranking module
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ store.py         # pgvector similarity search
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuration with RAG toggles
‚îÇ   ‚îú‚îÄ‚îÄ database.py          # SQLAlchemy async setup
‚îÇ   ‚îî‚îÄ‚îÄ models.py            # ORM models (Persona, ContentChunk)
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ build_coleai.py              # Build Cole Medin persona
‚îÇ   ‚îú‚îÄ‚îÄ test_github_only.py          # Quick test persona
‚îÇ   ‚îú‚îÄ‚îÄ rebuild_with_enhanced_rag.py # Rebuild with new RAG config ‚≠ê
‚îÇ   ‚îî‚îÄ‚îÄ test_rag_strategies.py       # Compare RAG configurations ‚≠ê
‚îú‚îÄ‚îÄ migrations/
‚îÇ   ‚îî‚îÄ‚îÄ 001_init_schema.sql          # Database schema
‚îú‚îÄ‚îÄ .env.example                     # Configuration template
‚îú‚îÄ‚îÄ docker-compose.yml               # PostgreSQL + pgvector
‚îú‚îÄ‚îÄ pyproject.toml                   # Poetry dependencies
‚îî‚îÄ‚îÄ README.md
```

### Data Flow

1. **Crawling** ‚Üí Content sources (GitHub, YouTube) ‚Üí Raw content
2. **Chunking** ‚Üí Header-based segmentation ‚Üí Semantic chunks
3. **Code Extraction** ‚Üí LLM summaries ‚Üí Indexed code blocks
4. **Embedding** ‚Üí OpenAI API (batch) ‚Üí 1536-dim vectors
5. **Storage** ‚Üí PostgreSQL + pgvector ‚Üí Vector search ready
6. **Query** ‚Üí User question ‚Üí Query embedding
7. **Retrieval** ‚Üí Vector similarity (3x if reranking) ‚Üí Candidate chunks
8. **Reranking** ‚Üí CrossEncoder scoring ‚Üí Top K results
9. **Generation** ‚Üí PydanticAI + Claude ‚Üí Final answer with citations

---

## üìà Performance & Optimization

### Indexing Performance

With enhanced RAG (header chunking + code extraction + reranking):
- **anthropic-sdk-python README**: 61 chunks + 17 code blocks = 78 items
- **Indexing time**: ~2-3 minutes (includes LLM calls for code summaries)
- **Embeddings**: Batch processing with exponential backoff (1s, 2s, 4s)

### Query Performance

| Metric | Basic RAG | Enhanced RAG | Impact |
|--------|-----------|--------------|--------|
| Response time | 34.38s | 46.31s | +35% latency |
| Retrieval quality | Good | Excellent | Significantly better |
| Code examples | Limited | Rich | 17 summarized blocks |
| Semantic accuracy | Good | Excellent | Improved via reranking |

### Cost Analysis

**Indexing Costs** (one-time per rebuild):
- Contextual embeddings: +$0.01-0.05 per 1000 chunks (gpt-4o-mini)
- Code summaries: +$0.01-0.03 per 100 code blocks (gpt-4o-mini)
- Embeddings: ~$0.13 per 1M tokens (text-embedding-3-small)

**Query Costs** (per query):
- Reranking: $0 (local CrossEncoder model)
- Embeddings: ~$0.0001 per query
- Claude 3 Opus: ~$0.01-0.05 per query (varies by response length)

---

## üßë‚Äçüíª Development

### Run Tests

```bash
# Run all tests
poetry run pytest

# Run with coverage
poetry run pytest --cov=src --cov-report=html
```

### Code Quality

```bash
# Format code
poetry run black src/ scripts/

# Lint
poetry run ruff check src/ scripts/

# Type checking
poetry run mypy src/
```

### Database Management

```bash
# Access PostgreSQL shell
docker exec -it persona_agents_db psql -U postgres -d persona_agents

# Check vector extensions
SELECT * FROM pg_extension WHERE extname = 'vector';

# View personas
SELECT id, name, slug FROM personas;

# View content chunks
SELECT id, persona_id, source_type, title FROM content_chunks LIMIT 10;
```

---

## üîß Adding New Personas

### Manual Approach

1. Create persona in database:
```python
from src.models import Persona
from src.database import AsyncSessionLocal

async with AsyncSessionLocal() as session:
    persona = Persona(
        name="Mike Israetel",
        slug="mike-israetel",
        bio="Exercise scientist and bodybuilding coach"
    )
    session.add(persona)
    await session.commit()
```

2. Crawl content:
```python
from src.crawlers.youtube import YouTubeCrawler

crawler = YouTubeCrawler(persona_id=str(persona.id))
contents = await crawler.crawl(channel_url="...")
```

3. Index content:
```python
from src.vector_store.store import VectorStore

for content in contents:
    await VectorStore.add_content(
        session=session,
        persona_id=str(persona.id),
        source_type="youtube",
        title=content["title"],
        content_text=content["content_text"],
        source_url=content["source_url"]
    )
```

### Automated Script (Template)

Create `scripts/build_<persona>.py`:
```python
import asyncio
from src.database import init_db, AsyncSessionLocal
from src.models import Persona
from src.crawlers.registry import get_crawler
from src.vector_store.store import VectorStore

async def build_persona():
    await init_db()
    async with AsyncSessionLocal() as session:
        # Create persona
        persona = Persona(name="...", slug="...", bio="...")
        session.add(persona)
        await session.commit()

        # Crawl and index
        crawler = get_crawler("youtube", str(persona.id))
        contents = await crawler.crawl(channel_url="...")

        for content in contents:
            await VectorStore.add_content(session, ...)

if __name__ == "__main__":
    asyncio.run(build_persona())
```

---

## üôè Credits & References

### Inspiration

This project implements production RAG best practices from:
- **[Cole Medin's mcp-crawl4ai-rag](https://github.com/coleam00/mcp-crawl4ai-rag)** - Production RAG implementation with Supabase
- **[Cole Medin's YouTube Channel](https://www.youtube.com/@ColeMedin)** - AI Agents Masterclass

### Key Concepts Learned

- **Header-based chunking** for better semantic boundaries
- **Code block extraction** with LLM summaries for agentic RAG
- **CrossEncoder reranking** for post-retrieval quality improvement
- **3x fetch strategy** (retrieve more, rerank, return top K)
- **Batch processing** with exponential backoff for fault tolerance
- **Toggleable RAG strategies** for production flexibility

### Technologies

- **[pgvector](https://github.com/pgvector/pgvector)** - Vector similarity search for PostgreSQL
- **[PydanticAI](https://ai.pydantic.dev/)** - Type-safe AI agent framework
- **[sentence-transformers](https://www.sbert.net/)** - CrossEncoder models for reranking
- **[Anthropic Claude](https://www.anthropic.com/claude)** - LLM for agents and summaries
- **[OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)** - text-embedding-3-small

---

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.

---

## ü§ù Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## üìû Support

- **Issues**: [GitHub Issues](https://github.com/gpt153/ColeAI/issues)
- **Discussions**: [GitHub Discussions](https://github.com/gpt153/ColeAI/discussions)

---

**Built with ‚ù§Ô∏è using Cole Medin's production RAG best practices**
