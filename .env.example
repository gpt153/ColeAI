# Database
DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/persona_agents

# OpenAI (for embeddings and LLM calls)
OPENAI_API_KEY=sk-...

# Anthropic (for PydanticAI agent)
ANTHROPIC_API_KEY=sk-ant-...

# PydanticAI Model (for agents)
PYDANTIC_AI_MODEL=anthropic:claude-3-opus-20240229

# GitHub (for crawling repos)
GITHUB_TOKEN=ghp_...

# MCP Server
MCP_SERVER_PORT=8100

# HTTP API
API_PORT=8000
API_HOST=0.0.0.0

# Logging
LOG_LEVEL=INFO

# ===== RAG Strategy Toggles (Cole Medin's Best Practices) =====

# Contextual Embeddings: Enhances each chunk with LLM-generated context
# Improves retrieval quality but slower indexing
USE_CONTEXTUAL_EMBEDDINGS=false

# Reranking: Uses CrossEncoder to rerank results after vector search
# Significantly improves relevance but adds latency
USE_RERANKING=false

# Code Extraction: Extracts and indexes code blocks separately with LLM summaries
# Better code search but requires more processing
USE_CODE_EXTRACTION=false

# Header Chunking: Intelligently chunks by markdown/HTML headers
# Better semantic boundaries than fixed-size chunking
USE_HEADER_CHUNKING=false

# LLM Model for contextual embeddings and code summaries
# Recommended: gpt-4o-mini for cost-effectiveness
CONTEXT_MODEL=gpt-4o-mini

# CrossEncoder model for reranking
RERANKING_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Minimum code block length for extraction (characters)
MIN_CODE_BLOCK_LENGTH=300

# Embedding settings
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSION=1536

# Chunking settings (for fixed-size mode)
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
